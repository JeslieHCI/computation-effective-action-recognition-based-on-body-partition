\documentclass[conference]{IEEEtran}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
\else
\fi

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{CJK}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{url}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{clrscode}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\ifCLASSOPTIONcompsoc
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
\usepackage[caption=false,font=footnotesize]{subfig}
\fi

\graphicspath{{motion/}}

\begin{document}

\title{Partition-based Human Action Recognition by Kinect}

\author{\IEEEauthorblockN{Li Jiawei}
	\IEEEauthorblockA{Nan Jing Univ of Posts\\ and Telecommunications\\
		Nan Jing, Jiang Su\\
		Email: B13010426@njupt.edu.cn}
}

\maketitle

\begin{abstract}
	Human action recognition is an important area of computer vision research while a challenging task because of the viewpoint variation and self-occlusion etc.
	The recent introduced cost-effective depth sensor bring about new opportunities to deal with these problems by providing accurate 3D joint locations.
	In this paper, we propose a novel approach to human action recognition based on joints partition.
	We use DTW distance and variance of each joint to represent the motion pattern.
	According to the analysis of actions, we divide joints which have the similar motion pattern into the same group.
	Since each part has different significance in actions, we compute the weight of each part to different actions.
	Then we represent skeletons as points in the lie group, use DTW to deal with the rate variation and classify each group using one-vs-all SVM classifier respectively.
	The action label is obtained by weighing the SVM classifier output of each part.
	We collected a dataset which has obvious intra-class variations.
	Our method performed well on our dataset.
	We also tested the proposed approach on two public action recognition datasets captured by Kinect.
	The experimental results show that our approach performs relatively better than most of the state-of-art algorithms.
\end{abstract}
 
\IEEEpeerreviewmaketitle

\section{INTRODUCTION}
Human action recognition has a lot of applications, such as surveillance systems, security, video games and robotics.
There are two main processes in the human action recognition system.
The first one is extracting features from the video sequence and reasonably representing the human body.
The other one is learning and recognizing actions from the action sequences.
Due to the variations among different instances of the same action, the differences in the viewpoints, self-occlusion and the variant rate of the actions, human action recognition is challenging.

Human action recognition began in the early of 1980s.
The previous research is mainly based on the traditional RGB video camera \cite{the_review_of_action_recognition_on_rgb_video_camera}.
However, the RGB video camera is extremely sensitive to the changing of brightness, viewpoints, and self-occulsion.
These disadvantages result in the motion captured from RGB video cameras losing a large volume of information, and decrease the accuracy of video-based action recognition.

With the recent introduction of cost-effective depth sensors, such as Mircosoft Kinect and ASUS Xtion PRO, depth images can be obtained much easier than before.
In \cite{pose_recognition_from_single_delth_images}, Shotton proposed an approach which can predict human pose and acquire the 3D location of each joint from a single depth image.
Using this technique, the extracted 3D data is real-time and more accurate, so it is helpful to overcome these challenges mentioned above.
Several apporaches based on 3D location of body joint have been proposed \cite{the_review_of_action_recognition}.

Although the 3D data obtained from the depth sensor is more accurate than that obtained from the traditional RGB video camera, there still have some problems when it comes to the intra-class variations in the motion.
For example, when someone is waving, we judge the action is waving only depending on the motion of arms.
The movement of feet or head are meaningless during the action of waving.
It is the same with the action such as sitting down, since we pay more attention to legs than other parts.
These factors indicate that if training samples contain all parts of the body, these parts make no contribution to recognize will decrease the accuracy of the action recognition system.
Most approaches lack capacities to conquer the intra-class variations, which result in the waste of computing resources and reducing the accuracy of identification.

In this paper, we propose an action recognition approach based on partitioning the joints reasonably.
Through the observation of actions, we found that human body parts during activities can be divided into two categories: significant parts and insignificant parts.
Here, significant parts are these which can be directly recognized or inferred based on the action sequences,
while the insignificant parts make no contribution to the recognition or even distract the recognition result.
Based on these findings, we employ a new evaluation approach to obtain the most reasonable partition scheme.
DTW \cite{introduction_on_dtw} (Dynamic time warping) distance and variance are widely used to reflect characters of a set of data.
DTW distance is able to reveal the difference between several sequences of each action,
and the variance is used to compute the dispersion degree of each joint angle in the same action.
Here we use the DTW distance and variance of joint angle to represent the motion pattern of joints.
After obtaining these representations, the joints which have the similar motion pattern can be partitioned into the same group.
According to the partition scheme, we compute the weight of each part for every actions, which reflects the significance degree of each part in different actions.

Then, we represent each part of the body as points in lie group respectively, following the instruction of Vemulapalli \cite{lie_group}.
To combat the problem of rate variations, we use DTW following the approach in \cite{Rate_invariant}.
To remove most of the high-frequency noise, we use Fourier temporal pyramid representation\cite{fourier_temporal_pyramid}.
After these preprocessing, we build action recognition classifier for each body part respectively through one-vs-all SVM classifier \cite{prml}.
The action with the highest value of the weighted scores is the result.
Fig. \ref{fig:sketch map} is the sketch map of the proposed approach.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\linewidth]{sketch_map.pdf}
	\caption{Sketch map of the proposed approach. A instance of action \textit{walking} is shown at the top of the figure, from the UTKinect dataset. As depicted in the Section \ref{sec:Motion Analysis}, in this example, the body are divided into $ M=3 $ parts. Each part is processed respectively, and the distance between sample point and hyperplane $ D_{m}\in R^{1\times 10} $ is obtained, which UTKinect dataset contains ten actions. Then we use $ W_{m} $ to weight each $ D_{m} $. Since the action \textit{walk} have the largest distance in the $ S $, which is highlighted by the red circle, the action is recognized as \textit{walk}.}
	\label{fig:sketch map}
\end{figure}

% ****************************************************************************************************************
Our main contribution consists of three aspects.
First, we proposed a new action recognition approach based on joints partition.
Second, we proposed a novel method to represent the motion pattern of joint using the variance and DTW distance of joint angles.
We also compute the weight of each part and it is used to overcome the intra-class variations.
Third, we collect a dataset which each subject perform actions in different motion patterns.

This paper is organized as followsï¼š
In section \ref{sec:RELATED WORKS}, we provide a brief review of the related work.
Section \ref{sec:Motion Analysis}  discusses the proposed approach to represent the motion pattern and analyses it;
Section \ref{sec:Algorithm Description} depicts the method to represent the body and classify, including the approach to calculate the weights of each part.
The experimental results are given in section \ref{sec:EXPERIMENTAL EVALUATION}.
Finally, section \ref{sec:CONCLUSION} is the conclusion of this paper.

% ****************************************************************************************************************

\section{RELATED WORKS}
\label{sec:RELATED WORKS}
There are many approaches been proposed for human action recognition in the past few years.
It is generally known that using 3D location of body joint can obtain better performance for action recognition.
In \cite{multi_cameras_obtain_3D_data}, the sophisticated motion capture system provided the 3D position of labels placed on the human body, but such equipments are very expensive.
With the recent introduction of cost-effective depth sensors, obtaining 3D location of joint becomes easier than before.
But there are still many challenges to vision-based human action recognition,
such as viewpoint, intra-class variations and inter-class similar.

The first challenge is the view variations.
The same action has various appearance from different viewpoints.
Most of state-of-the-art approaches use the relation of joints or the joint angle to represent the skeleton for solving this problem.
In \cite{hostogram_of_3d_joints}, skeletons were rotated to make the vector from the left hip to the right hip parallel to the x-axis of the coordinate, so the skeleton is view invariant.
The classification was performed by HMMs.
In \cite{Sequence_of_the_most_informative_joints}, Ofli chose some joints as the most informative joints, and represented the body as a sequence of these most informative joints.
Since this approach used the variation of joint angle, the changing of viewpoint does not affect the recognition accuracy.
Georgios \cite{skeletal_quads} used the skeletal quad as the descriptor which encoded the geometric relation of joint quadruples.
The rotation around local coordinate will be normalized, so this approach is still view-invariant.

The second challenge is to overcome the intra-class variations.
Since individuals can perform the same action in different motion pattern, it will be hard for machine to recognize two instances of the same action which have different motion patterns.
In \cite{learning actionlet ensemble}, Jiang Wang proposed a novel actionlet ensemble model which represented the interaction of a subset of joints.
They used Fourier Temporal Pyramid to remove the noise of depth data.
Vemulapalli et al. \cite{lie_group} proposed a new apporach that modeled the 3D geometric relationship between various body parts using special Euclidean group $ SE(3) $ and represented the skeleton in Lie group $ SE(3) \times...\times SE(3) $.
Xiaodong Yang \cite{Eigenjoints} described the difference of each joint in temporal and spatial domains, and applied PCA to find the Eigenjoints.
The classification was performed by NBNN.
In \cite{pose_based}, Wang grouped the estimated joints into five body parts and used it to represent the human body.
This approach is robust to errors on joint estimation and intra-class variations.
Mikel \cite{MACH} proposed an approach based on a Maximum Average CorrelationHeight filter, which is able to capture intra-class variability by synthesizing a single Action MACH filter for a given action class.
% **************************************************************************************************************************************
However, most of state-of-the-art works mainly concentrate on finding an representation to eliminate the interference of intra-class variations.
In our work, we attempt to segment joints according to motion pattern and classify the action for each class respectively and evaluate the proposed approach on multiple datasets.

\section{MOTION PATTERN ANALYSIS}
\label{sec:Motion Analysis}
	We use the absoulte joint angle to evaluate the motion pattern of each joint, which is the angle between two directly connected bones.
	Fig. \ref{fig:skeleton_joints} depicts the structure of skeleton and absolute joint angles used in this paper.
	In this body model, there are twenty joints, nineteen body parts and eighteen absolute joint angles.
	The name of each absloute joint angle is shown on the left of the Fig. \ref{fig:skeleton_joints}.
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=2.4in]{skeleton_angles.pdf}
		\caption{The structure of the skeleton used in this paper}
		\label{fig:skeleton_joints}
	\end{figure}
	\subsection{Computing the DTW Distance and Variance of Joints}
	\label{sec:dtw and variance}
		DTW is a well-known technique used to find the optimal alignment between two time-dependent sequences.
		The first step is to compute the cost matrix between two angle sequences.
		Let $ C\in\mathbb{R}^{N\times M} $ be the cost matrix, which is obtained from the euclidean distance between the suquences $ X $ and $ Y $.
		Here, $ X=(x_{1},x_{2},...,x_{N}) $ and $ Y=(y_{1},y_{2},...,y_{M}) $ are two angle sequences, where $ N $ and $ M $ are the length of sequences.
		According to the cost matrix $ C $, the accumulated cost matrix $ D $ can be obtained as
		\begin{equation}
			\left\{
			\begin{array}{l}
			D(n,1)=\sum_{k=1}^{n}C(k,1) \\
			D(1,m)=\sum_{k=1}^{m}C(1,k) \\
			D(n,m)=\min\{D(n-1,m-1),D(n-1,m), \\
			~~~~~~~~~~~~~~~~~~~~~~~~~~D(n,m-1)\}+c(x_{n},y_{m})
			\end{array}
			\right.,
		\end{equation}
		where $ 1\leq n\leq N $ and $ 1\leq m\leq M $.
		Then, the optimal warping path $ P=(p_{1},...,p_{L}) $ is
		\begin{equation}
			p_{l-1}=\left\{
			\begin{array}{ll}
			(1,m-1),& if~n=1 \\
			(n-1,m),& if~m=1 \\
			argmin\{D(n-1,m-1), & \\
			D(n-1,m),D(n,m-1)\},& otherwise
			\end{array}
			\right..
		\end{equation}
		Here $ p_{l} = (n, m) $ is the element in the optimal warping path and $ L $ is the length of the path.
		The distance can be calculated by summing elements of optimal alignment $ P $ in the cost matrix $ C $.
		It is represented by the equation
		\begin{equation}
			d_{n}^{r}=\sum_{i=1}^{T}\sum_{j=1}^{L}C(p_{j}),
		\end{equation}
		where $ n $ is the action number, $ r $ is the joint number, $ T $ is the instance number of actions and $ L $ is the length of the optimal warping path.

		The variance of a sequence $ \Theta $ is defined as $ Var(\Theta) $. The variance of angles is extracted by
		\begin{equation}
			{\sigma}_{n}^{r}=\sum_{i=1}^{T}Var(\Theta_{i}),
		\end{equation}
		where $ \Theta=\{\theta_{1},\theta_{2},...,\theta_{T}\} $ is the angle sequence of an action instance, $ F $ is the number of frames.
		\begin{figure}[h]
			\centering
			\subfloat[]{
				\centering
				\label{fig:angles_variance}
				\includegraphics[width=3.5in]{angles_var.pdf}}
			\vfill
			\centering
			\subfloat[]{
				\centering
				\label{fig:dtw_distance}
				\includegraphics[width=3.5in]{dtw_distance.pdf}}
			\caption{(a) the variance matrix of UTKinect dataset; (b) the DTW distance matrix of UTkinect dataset}
			\label{fig:motion_pattern_matrix}
		\end{figure}
	\subsection{Motion Pattern Representation and Analysis}
		According to the above definition,
		we represent the motion pattern of UTKinect dataset \cite{hostogram_of_3d_joints}.
		Fig. \ref{fig:angles_variance} is the variance matrix and Fig. \ref{fig:dtw_distance} is the DTW distance matrix of the UTKinect dataset.
		The color tone represents the variances and the DTW distances.
		On the other hand, the color tone can be regarded as the motion pattern.
		In the Fig. \ref{fig:angles_variance}, we find that some joints have the similar color pattern.
		Fro example, the joint angle \textit{right arm} and \textit{right elbow} have the similar color pattern in the different action.
		It is the same to other joints such as \textit{left arm} and \textit{left elbow}.
		%Note that, since the angle of \textit{right wist, left wist, right foot and left foot} are not accurate, we did not use them.
		
\section{Algorithm Description}
\label{sec:Algorithm Description}
		In the proposed approach, we divide joints into different classes, and approach the classifier for each class respectively.
		Through weighting each part, the interference of intra-class variations is better eliminated from the results.
		Based on the analysis of Section. \ref{sec:Motion Analysis}, we divide the joints which have the similar motion pattern into the same class.
		For example, Fig. \ref{fig:motion_pattern_matrix} depicts the variance matrix and DTW matrix of UTKinect dataset.
		It is clear that some joints, such as \textit{chest1, chest2, chest3 and spine}, or \textit{right arm, right elbow, left arm and left elbow}, have the similar motion pattern in different action, so we can divide these joints into the same class.
		Each class will be represented in the lie group respectively.
		Then we use DTW to handle the different action rate and Fourier Temporal Pyramid to overcome the noise.
		Classifier of each class is realised by one-vs-all SVM classifier.
		The final action label is obtained by weighted the SVM classifier output of each class.
	\subsection{3D Representation}
		We represent 3D skeletons as points in a lie group \cite{introduction_on_SE3} to improve the accuracy of recognition, following the instruction in \cite{lie_group}.
		Fig. \ref{fig:skeleton_joints} shows a skeleton which have 20 joints and 19 body parts.
		For example, there are two body parts $ s_{m} $ and $ s_{n} $.
		Starting and end points are $ s_{m1},s_{n1}\in R^3 $ and $ s_{m2},s_{n2}\in R^3 $.
		The first step is to convert the global coordinate into the local coordinate.
		Local coordinate systems belong to $ s_{m},s_{n} $ are $ \chi_{m},\chi_{n} $.
	
		Let the starting and end points of the skeleton $ s_{m} $ at time instance $ t $ are $ s_{m1,t}^n,s_{m2,t}^n\in R^3 $, which are represented in the local coordinate system.
		The $ s_{m1,t}^n,s_{m2,t}^n $ can be obtained by
		\begin{equation}
			\left[
				\begin{array}{cc}
					s_{m1,t}^n & s_{m2,t}^n \\
					1 & 1
				\end{array}
			\right]=\left[
				\begin{array}{cc}
					R_{mn,t} & \vec{p_{mn,t}} \\
					0 & 1
				\end{array}
			\right]\left[
				\begin{array}{cc}
					0 & l_{m} \\
					0 & 0 \\
					0 & 0 \\
					1 & 1 \\
					\end{array}
			\right],
		\end{equation}
		where $ l_{m} $ is the length of skeleton $ s_{m} $.
		$ R_{mn,t} $ and $ p_{mn,t} $ are the rotation matrix and translation matrix at time $ t $.
		The relative features between $ s_{m} $ and $ s_{n} $ are represented as
		\begin{equation}
			g_{n,m}(t)=\left[
			\begin{array}{cc}
			R_{nm,t} & \vec{p_{nm,t}} \\
			0 & 1
			\end{array}
			\right]\in SE(3),
		\end{equation}	
		\begin{equation}
			g_{m,n}(t)=\left[
				\begin{array}{cc}
					R_{mn,t} & \vec{p_{mn,t}} \\
					0 & 1
				\end{array}
			\right]\in SE(3).
		\end{equation}
		
		The combination of multiple $ SE(3) $ can be obtained by direct product.
		Then we represent the skeleton $ S $ at time $ t $ using
		\begin{equation}
			\Psi(t)=[g_{1,2}(t)\times g_{2,1}(t)\times ...\times g_{N-1,N}(t)\times g_{N,N-1}(t)],
		\end{equation}
		where $ N $ is the number of body parts.
		Since the data belonging to $ SE(3) $ space cannot be used for the common classifier such as SVM, we need to map the $ SE(3)\times...\times SE(3) $ to $ se(3)\times...\times se(3) $.
		The features from special Euclidean Group are mapped to Lie algebra by
		\begin{equation}
			\begin{array}{r}
				\psi(t)=[\log(g_{1,2}(t))\times \log(g_{2,1}(t))\times ... \\
				\times\log(g_{N-1,N}(t))\times\log(g_{N,N-1}(t))].
			\end{array},
		\end{equation}
		which is a vector and can be used in the action recognition system.
		
	\subsection{Preprocessing and Building Classifier}
		Since the temporal rate changes in the executions of an action, it is necessary to preprocess the skeleton data.
		Each instance of the same action having different frame numbers results in the rate variation.
		We use DTW to handle rate changes \cite{Rate_invariant}.
		The first instance of each action is set as the standard sequence, and the other instances are warped to have the same length with the standard sequece.
		And the depth map obtained by cost-effective depth camera sometimes has noisy and the extracted skeleton data is not always reliable.
		Here we use Fourier Temporal Pyramid representation \cite{fourier_temporal_pyramid} to deal with the noise.
		This representation segments an action sequence into pyramid, and uses short Fourier transform for all the segments.
		The final feature just use the low frequency Fourier coefficient, so it is robust to the high frequency noise.
		Then we approach action recognition classifier through one-vs-all SVM classifier \cite{prml} for each class respectively.

		\begin{figure}[h]
			\centering
			\subfloat[original distribution]{
				\centering
				\label{fig:real distribution}
				\includegraphics[width=3in]{realDistribution.pdf}}
			\vfill
			\centering
			\subfloat[weighted distribution]{
				\centering
				\label{fig:weighted distribution}
				\includegraphics[width=3in]{ideaDistribution.pdf}}
			\caption{(a)The original distribution of the action \textit{walking}, in UTKinect dataset. These points have three dimension; (b)The weighted one-dimension points, transformed from the original points.It is clear that almost entire positive points are greater than zero and neative points are on the left of zero point.}
		\end{figure}

	\subsection{Weight Computation}
		In the proposed approach, we use one-vs-all SVM to approach the classification.
		One-vs-all SVM classifier is the construction of multiclass SVMs, and the output is a list of confidence score of each SVM.
		The predicting label is the corresponding classifier which reports the highest confidence score.
		Typically, the classifier corresponding to the label has a positive score, while other classifiesr have a negative score.
		The proposed approach divides a body into several classes, the result is the combination of these SVM classifiers.
		Fig. \ref{fig:real distribution} is the distribution of the action \textit{walking}, which divides the body into three classes and tests on the UTKinect dataset.
		The distribution has three dimension because the body is divided into three classes.
		
		Take Fig. \ref{fig:real distribution} for instance.
		There are two kinds of points in the figure, positive points $ P_{posi} $ and negative point $ P_{nega} $.
		We define the point of which input and classifier have the same label as positive point, and of which have the different label as negative point.
		After weighting, sample points are transformed from three dimension to one deminsion.
		The ideal result is that positive points is larger than negative points, or the value of positive points is positive and that of negative points is negative.
		Fig. \ref{fig:weighted distribution} depicts the weighted distribution of original data.
		It corresponds the above assumption.
		
		Here, we partly refer to the logistic regression to approach this target.
		Algorithm \ref{alg:compute the weight} describe the approach used to compute the weight.
		In each iteration, the weight $ w $ will be refreshed by $ w = w + \lambda\nabla f(w) $, where $ \lambda $ is the step length and $ \nabla f(w) $ is the gradient of the object function $ f(w) $.
		The loop will be continued until times of iteration are greater than the threshold or the gradient is less than tolerance.
		In the Algorithm \ref{alg:compute the weight}, we define the sigmoid function as
		\begin{equation}
			\sigma(z) = \frac{1}{1+e^{-z}},
		\end{equation}
		where $ z = w_{0} + w_{1}x_{1} + ... + w_{M}x_{M} $ and $ M $ is the number of classes.
		The cost function is defined by
		\begin{equation}
			f(w) = \frac{1}{T}\sum_{i=1}^{T}[-y_{i}\log(\sigma(z_{i})) - (1-y_{i})\log(1-\sigma(z_{i}))],
		\end{equation}
		Where $ T $ is the number of the instance.
		And the gradient of cost function is obtained by
		\begin{equation}
			\nabla f(w) = \frac{1}{T}\sum_{i=1}^{T}(\sigma(z_{i}) - y_{i})x_{i}.
		\end{equation}
		\begin{algorithm}
			\caption{Weight Computation}
			\label{alg:compute the weight}
			\begin{algorithmic}
				\REQUIRE Cost function $ f(w) $, gradient function $ \nabla f(w) $, maximum number of iterations $ maxiter $, tolerance $ \varepsilon $.
				\ENSURE Weight matrix $ w $
				\STATE\textbf{Initialization:} $ w = [0~...~0], k = 0 $
				\WHILE {$ k \textless maxiter $}
				\STATE Compute gradient $ \nabla f(w) $;
				\IF { $ \nabla f(w) $ \textless $ \varepsilon $}
					\STATE \textit{break}
				\ENDIF
				\STATE Find $ \lambda $ which minimize $ f(w + \lambda\nabla f(w)) $
				\STATE $ w = w + \lambda\nabla f(w) $;
				\STATE $ k = k + 1 $;
				\ENDWHILE
				\RETURN $ w $
			\end{algorithmic}
		\end{algorithm}
			
	\subsection{Classification}
		As shown in Fig. \ref{fig:sketch map}, after obtaining the distance $ d_{n,m} $ between hyperplane and sample point from the SVM classifier of each class, the distance $ D $ is weighted by $ W $. The weighted distance $ S $ is
		\begin{equation}
			S_{n}=\sum_{i=1}^{M}d_{n,i}W_{n,i}.
		\end{equation}
		Then, the action label is obtained by
		\begin{equation}
			label=\mathop{\arg\max}_{1\leq n\leq N}{S_{n}}.
		\end{equation}

\section{PERFORMANCE EVALUATION}
\label{sec:EXPERIMENTAL EVALUATION}
	We evaluate the proposed approach using a dataset collected by ourselves. It is a challenging dataset due to the self-occulsion and intra-class variations.
	In addition, we compare it on the public UTKinect \cite{hostogram_of_3d_joints} dataset and MSR Actions3D dataset \cite{MSR3D_dataset}.
	We also evaluate the different partition scheme.
	\subsection{Dataset}
		(1) In order to evaluate the performance, a Kinect is used to capture RGB images and depth maps. It consists of ten actions performed by ten subjects in indoor settings. Each subject repeats the same action twice. 3D locations of 25 joints are available from Kinect SDK. This dataset contains ten acitons: \textit{walk, pick up, read book, sit down, stand up, jump, clap, make phone call, throw and drink}. We use 3D location of joints for action recognition. The RBG images and depth maps are not used.
		
		As shown in the Tab. \ref{tab:walk}, we realize the same action with different motion pattern to highlight the ability of the proposed approach to overcome the problem of intra-class variations.
		For example, in the action walking, some actors walk with their arms sagging naturally, while others walk with hands crossing the chest.
		And action sequences have different frames.
		These factors both increase the difficulty of action recognition.
		\begin{table}[htbp]
			\caption{sample images of four subjects performing the action \textbf{walking}}
			\label{tab:walk}
			\begin{center}
				\begin{tabular}{|c|c|} \hline
					Subject No. & Walking sequence \\\hline
					Subject A & \parbox[c]{2.4in}{\includegraphics[width=2.4in]{walk_p1.pdf}} \\\hline
					Subject B & \parbox[c]{2.4in}{\includegraphics[width=2.4in]{walk_p2.pdf}} \\\hline
					Subject C & \parbox[c]{2.4in}{\includegraphics[width=2.4in]{walk_p7.pdf}} \\\hline
					Subject D & \parbox[c]{2.4in}{\includegraphics[width=2.4in]{walk_p9.pdf}} \\\hline
				\end{tabular}
			\end{center}
		\end{table}
		
		(2) UTKinect dataset \cite{hostogram_of_3d_joints}:
		Videos were captured using one Kinect with Kinect for Windows SDK Beta Version.
		This dataset consists of 10 actions: \textit{walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands and clap hands}.
		The dataset contains 10 subjects and each subject performs each action twice.
		There are 199 action sequences in this dataset.
		Three channels were recorded: RGB, depth and skeleton joint locations.
		The frame rate is 30 frames per second and the three channels are synchronized.
		The 3D locations of 20 joints are provided in this dataset.
		It is a challenging dataset becasue of the variation in the viewpoint.
		
		(3) MSR-Action3D dataset \cite{MSR3D_dataset}:
		Videos were captured using a stationary Kinect.
		There are 20 actions: \textit{high arm wave, horizontal arm wave, hammer, hand catch, forward punch, high throw, draw x, draw tick, draw circle, hand clap, two hand wave, side-boxing, bend, forward kick, side kick, jogging, tennis swing, tennis serve, golf swing, pick up and throw}.
		Each action was performed by seven subjects for three times.
		The depth maps were captured at about 15 frames per second by a depth camera that acquires the depth through infrared light.
		The size of the depth map is 640 by 480.
		Altogether, the dataset has 23797 frames of depth maps for 4020 action samples.
		
	\subsection{Experimental Results}
		Our experiment are divided into two parts. 
		In the first part, we evaluate our algorithm on the UTKinect dataset \cite{hostogram_of_3d_joints} with different partition schemes.
		In the second part, we assess the approach on three different datasets using the relatively best partition scheme, which is based on the result of part one.
		\subsubsection{Expriment Results on the Different partition Schemes}
			Based on the conclusion of section \ref{sec:Motion Analysis}, we divide joints which have the samiliar motion pattern into the same class.
			Fig. \ref{fig:angles_variance} and Fig. \ref{fig:dtw_distance} reveal that spine part, arms part and leg part have the similar motion pattern.
			The number of each joint is shown in the Fig. \ref{fig:skeleton_joints}.
			Since it is difficult to make a quantitative analysis on these figures, we evaluate the four alternative partition scheme:
			
			\textbf{a) one class:} entire body in one class.
			
			\textbf{b) two classes:} \textit{upper body} class and \textit{legs} class.
			
			\textbf{c) three classes:} \textit{arms} class, \textit{legs} class and \textit{spine} class.
			
			\textbf{d) four classes:} \textit{left arm} class, \textit{right arm} class, \textit{left leg} class, \textit{right leg} class and \textit{spine} class.
			
			\begin{figure}[h]
				\centering
				\includegraphics[width=3.6in]{scheme_analysis.pdf}
				\caption{Recognition rate of different partition scheme on UTKinect}
				\label{fig:scheme_analysis}
			\end{figure}
			
			\begin{table}[htbp]
				\caption{recognition rate of three subsets for msr3d dataset}
				\label{tab:MSR3D}
				\begin{center}
					\begin{tabular}{|l|c|c|c|c|} \hline
						Subset       & $AS_{1}$   & $AS_{2}$ & $AS_{3}$ & Average \\\hline
						Recognition rate & 86.37\% & 75.56\% & 96.98\% & 86.31\% \\\hline
					\end{tabular}
				\end{center}
			\end{table}
			\begin{table}[htbp]
				\caption{comparison with the state-of-art results}
				\label{tab:utkinect and msr3d results comparation}
				\begin{center}
					\begin{tabular}{|l|c|} \hline
						\multicolumn{2}{|c|}{A: UTKinect dataset \cite{hostogram_of_3d_joints}} \\\hline
						Histograms of 3D joints \cite{hostogram_of_3d_joints} & 90.92\% \\\hline
						Random forests \cite{Fusing_spatiotemporal_features_and_joints} & 87.9\% \\\hline
						relative pairs in lie group \cite{lie_group} & 97.08\% \\\hline
						Histogram of Direction Vectors \cite{natural_human_robot}& 91.96\% \\\hline
						proposed algorithm & 95.67\% \\\hline
						\hline
						\multicolumn{2}{|c|}{B: MSR3D dataset \cite{MSR3D_dataset}} \\\hline
						Histograms of 3D joints \cite{hostogram_of_3d_joints} & 78.97\% \\\hline
						EigenJoints \cite{Eigenjoints} & 83.8\% \\\hline
						proposed algorithm & 86.31\% \\\hline
					\end{tabular}
				\end{center}
			\end{table}
			\begin{figure}[h]
				\centering
				\subfloat[]{
					\centering
					\label{fig:UTKinect confusion matrix}
					\includegraphics[width=2.7in]{UTKinect_2.pdf}}
				\vfill
				\centering
				\subfloat[]{
					\centering
					\label{fig:AS2 confusion matrix}
					\includegraphics[width=2.7in]{AS2.pdf}}
				\vfill
				\centering
				\subfloat[]{
					\centering
					\label{fig:my dataset confusion matrix}
					\includegraphics[width=2.7in]{my_dataset.pdf}}
				\caption{Confusion matrices: (a) UTKinect dataset; (b) dataset collected by ourself}
			\end{figure}
			Fig. \ref{fig:scheme_analysis} is the result of different partition scheme on the UTKinect dataset.
			Compared with the result of no-partition, actions such as \textit{sit down, stand up, push, pull and wave hand} obtain better performance when joints are divided.
			This is because these actions are	 related to certain parts of the body, and weighting of each part further eliminate the interference from insignificant parts.
			Take the action \textit{sit down} for instance.
			Sitting down can be regarded as a process which people bend their knees and then sit on the chair, while the motion of upper body make no sense.
			The motion of legs is relatively certain, but upper body performs more disorderly.
			Through the proposed approach, the interference of the upper body can be largely alleviated.
			With the increasing of divided classes, the recognition rate of some actions, such as \textit{pick up, carry, throw}, perform not very well.
			Since these actions are more complex and have a relatively different motion pattern from other actions, the proposed partition scheme is not suitable in these conditions.
			In the end, we use the partition scheme which classifies joints into two classes.
			
		\subsubsection{Experiment Results on the Different Datasets}
			We evluate our algorithm on three different datasets and take the partition scheme which has two classes.
			
			Table.\ref{tab:utkinect and msr3d results comparation}-A lists the results of various human action recognition algorithms based on the 3D joint location over the UTKinect dataset.
			From the table, the proposed approach is better than most of state-of-the-art algorithms.
			But, our average recognition rate is 1.41\% lower than that of \cite{lie_group}.
			This is mainly because the recognition rate ofaction \textit{throw} is 73.33\% and it lowers the average recognition rate.
			Fig. \ref{fig:UTKinect confusion matrix} shows the confusion matrix of UTKinect dataset.
			We can see that the action \textit{throw} has been inaccurately classified as \textit{push, or clap}, since these actions are very similar.

			Table. \ref{tab:MSR3D} lists the recognition rate of MSR-Action3D dataset \cite{MSR3D_dataset}.
			Due to the large amount of data, we followed the instruction of \cite{MSR3D_dataset} and divieded the 20 actions into three subsets (AS1, AS2, AS3), each having 8 actions.
			The average recognition rate is 86.31\%.
			As the recognition rate of AS2 is poor, we make its confusion matrix for further analysis.
			We can see that the misclassification mainly occurs between the highly similar action \textit{hand catch, draw X, draw tick, draw circle} which only involve hands in the motions.
			Table.\ref{tab:utkinect and msr3d results comparation}-B shows that the proposed approach outperforms than many algorithms on the MSR3D dataset.
			
			Fig. \ref{fig:my dataset confusion matrix} depicts the confusion matrix of the dataset collected by ourselves.
			The average recognition rate is 87.57\%.
			It is clear that the propsed approach achieves very high recognition rate for most of the actions.
			But it does not work well on some actions like making a phone call or drinking.
			This is due to that the 3D joint location is not accurate in these actions due to the self-occlusion.
			All joints participating in these actions are so closed to each other that Kinect is difficult to avoid the interferece.
			And each subject has different procedure of making a phone call, such as taking the phone from differnet packet or using different hand.
			It increases the difficulty of recognition.
			
\section{CONCLUSION}
\label{sec:CONCLUSION}
	In this paper, we propose an action recognition approach based on joints partition.
	These joints which have the same motion pattern are classified into the same class.
	We also propose a new method to evaluate the weight of the each class.
	The classification of each class is performed respectively.
	The output of classifier of each class will be weighed to obtain the final result.
	Since the motion of some classes make no sense in some actions, this method uses the weight to lower the interference of these classes.
	In addition, we collect an action dataset which includes intra-class variations and self-occulsion.
	We evaluate the proposed approach on three different dataset and results show that our approach performs better than the state-of-the-art action recognition approaches based on the 3D location of joint.
	Results suggest that approaching classification on different classes respectively and weighted each class can effectively eliminate the interference of insignificant classes.
	
	In our work, we use DTW distance and joint variance to represent the motion pattern.
	In future, we aim to explore a more universal and flexible partition approach based on the meaning of actions.
	
\section*{Acknowledgment}
The authors would like to thank...

\begin{thebibliography}{1} 
\bibitem{the_review_of_action_recognition_on_rgb_video_camera}
Aggarwal J K, Ryoo M S. Human activity analysis: A review[J]. ACM Computing Surveys (CSUR), 2011, 43(3): 16.

\bibitem{the_review_of_action_recognition}
Aggarwal J K, Xia L. Human activity recognition from 3d data: A review[J]. Pattern Recognition Letters, 2014, 48: 70-80.

\bibitem{pose_recognition_from_single_delth_images}
Shotton J, Sharp T, Kipman A, et al. Real-time human pose recognition in parts from single depth images[J]. Communications of the ACM, 2013, 56(1): 116-124.

\bibitem{multi_cameras_obtain_3D_data}
Campbell L W, Bobick A E. Recognition of human body motion using phase space constraints[C]//Computer Vision, 1995. Proceedings., Fifth International Conference on. IEEE, 1995: 624-630.

\bibitem{learning actionlet ensemble}
Wang J, Liu Z, Wu Y, et al. Learning actionlet ensemble for 3D human action recognition[J]. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2014, 36(5): 914-927.

\bibitem{MACH}
Rodriguez M D, Ahmed J, Shah M. Action MACH: A spatio-temporal maximum average correlation height filter for action recognition[C]//Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008: 1-8.

\bibitem{pose_based}
Wang C, Wang Y, Yuille A L. An approach to pose-based action recognition[C]//Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. IEEE, 2013: 915-922.

\bibitem{offset_of_3D}
Lu G, Zhou Y, Li X, et al. Efficient action recognition via local position offset of 3D skeletal body joints[J]. Multimedia Tools and Applications, 2015: 1-16.

\bibitem{lie_group}
Vemulapalli R, Arrate F, Chellappa R. Human action recognition by representing 3d skeletons as points in a lie group[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 588-595.

\bibitem{introduction_on_dtw}
Muller M. Information retrieval for music and motion[M]. Heidelberg: Springer, 2007.

\bibitem{Sequence_of_the_most_informative_joints}
Ofli F, Chaudhry R, Kurillo G, et al. Sequence of the most informative joints (smij): A new representation for human skeletal action recognition[J]. Journal of Visual Communication and Image Representation, 2014, 25(1): 24-38.

\bibitem{Rate_invariant}
Veeraraghavan A, Srivastava A, Roy-Chowdhury A K, et al. Rate-invariant recognition of humans and their activities[J]. Image Processing, IEEE Transactions on, 2009, 18(6): 1326-1339.

\bibitem{fourier_temporal_pyramid}
Wang J, Liu Z, Wu Y, et al. Mining actionlet ensemble for action recognition with depth cameras[C]//Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012: 1290-1297.

\bibitem{prml}
Bishop C M. Pattern recognition and machine learning[M]. springer, 2006.

\bibitem{introduction_on_SE3}
Murray R M, Li Z, Sastry S S, et al. A mathematical introduction to robotic manipulation[M]. CRC press, 1994.

\bibitem{hostogram_of_3d_joints}
Xia L, Chen C C, Aggarwal J K. View invariant human action recognition using histograms of 3d joints[C]//Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on. IEEE, 2012: 20-27.

\bibitem{Fusing_spatiotemporal_features_and_joints}
Zhu Y, Chen W, Guo G. Fusing spatiotemporal features and joints for 3d action recognition[C]//Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on. IEEE, 2013: 486-491.

\bibitem{natural_human_robot}
Chrungoo A, Manimaran S S, Ravindran B. Activity Recognition for Natural Human Robot Interaction[M]//Social Robotics. Springer International Publishing, 2014: 84-94.

\bibitem{MSR3D_dataset}
Li W, Zhang Z, Liu Z. Action recognition based on a bag of 3d points[C]//Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on. IEEE, 2010: 9-14.

\bibitem{Eigenjoints}
Yang X, Tian Y L. Effective 3d action recognition using eigenjoints[J]. Journal of Visual Communication and Image Representation, 2014, 25(1): 2-11.

\bibitem{skeletal_quads}
Evangelidis G D, Singh G, Horaud R. Continuous gesture recognition from articulated poses[C]//Computer Vision-ECCV 2014 Workshops. Springer International Publishing, 2014: 595-607.

\end{thebibliography}

% that's all folks
\end{document}

